# Real-time-detection-of-ASL-Alphabets
The users of American Sign Language range somewhere between 250,000 to 500,000 persons. But most of the communication happens among the persons suffering from deafness and those who have learned the American Sign Language. If someone who is not proficient wants to have communication, then he or she needs someone who has learned the American Sign Language. There are experts in sign language who act as moderators between the person who is verbally speaking the disabled person who is using the sign language.

In this project, a deep learning model is built to alleviate the problem a bit. Deep learning and convolutional neural networks is used to build a model that can recognize the American Sign Language alphabets with decent enough accuracy.

**Link to the dataset** - https://www.kaggle.com/grassknoted/asl-alphabet

### Conclusion:
The proposed model is doing good at predicting the letters. But still, we can see a lot of fluctuation in the real-time predictions. 
Even a slight hand movement is making the neural network to change its predictions. This is also called as â€˜flickering'.

### Future work: 
- Creating full sentences  instead of just alphabets
- Customise model for all sign languages
- Creating automatic editors
